{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **WEB SCRAPING**"
      ],
      "metadata": {
        "id": "-_OJVGG22X0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import sqlite3\n",
        "import xml.etree.ElementTree as ET\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import json"
      ],
      "metadata": {
        "id": "Hpq-4nUu1FQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Database Setup **"
      ],
      "metadata": {
        "id": "FeGGXpgx1LyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_db(db_path):\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    cursor.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS pages (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        url TEXT UNIQUE,\n",
        "        title TEXT,\n",
        "        content TEXT\n",
        "    );\n",
        "    \"\"\")\n",
        "\n",
        "    cursor.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS page_sections (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        url TEXT,\n",
        "        section_heading TEXT,\n",
        "        section_content TEXT\n",
        "    );\n",
        "    \"\"\")\n",
        "\n",
        "    cursor.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS page_tables (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        url TEXT,\n",
        "        row_content TEXT\n",
        "    );\n",
        "    \"\"\")\n",
        "\n",
        "    conn.commit()\n",
        "    return conn"
      ],
      "metadata": {
        "id": "w3O8Vd5W1GLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parsing Sitemaps**"
      ],
      "metadata": {
        "id": "BGzI4Loa1WBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_sitemap(sitemap_url):\n",
        "    urls = []\n",
        "    try:\n",
        "        response = requests.get(sitemap_url)\n",
        "        if response.status_code == 200:\n",
        "            root = ET.fromstring(response.content)\n",
        "            namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "            for child in root.findall('ns:sitemap', namespace):\n",
        "                loc = child.find('ns:loc', namespace)\n",
        "                if loc is not None:\n",
        "                    urls.extend(parse_nested_sitemap(loc.text))\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing sitemap index: {e}\")\n",
        "    return urls\n",
        "\n",
        "def parse_nested_sitemap(nested_url):\n",
        "    urls = []\n",
        "    try:\n",
        "        response = requests.get(nested_url)\n",
        "        if response.status_code == 200:\n",
        "            root = ET.fromstring(response.content)\n",
        "            namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "            for url in root.findall('ns:url', namespace):\n",
        "                loc = url.find('ns:loc', namespace)\n",
        "                if loc is not None:\n",
        "                    urls.append(loc.text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing nested sitemap: {e}\")\n",
        "    return urls\n",
        "\n",
        "def extract_all_urls(sitemap_url):\n",
        "    return parse_sitemap(sitemap_url)"
      ],
      "metadata": {
        "id": "dwvclN3a1Wc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check for any Duplicates if re-scraping"
      ],
      "metadata": {
        "id": "zSvJDDPD1nVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_url_in_db(conn, url):\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\"SELECT 1 FROM pages WHERE url = ?\", (url,))\n",
        "    return cursor.fetchone() is not None"
      ],
      "metadata": {
        "id": "bD7Sl8ZK1m3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Content Extraction**"
      ],
      "metadata": {
        "id": "lZmQgzAy1yzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_main_content(soup):\n",
        "    return soup.get_text(separator=\"\\n\", strip=True)\n",
        "\n",
        "def extract_sections(soup):\n",
        "    sections = []\n",
        "    for heading in soup.find_all(['h2', 'h3']):\n",
        "        section_heading = heading.get_text(strip=True)\n",
        "        section_content = ''\n",
        "        for sibling in heading.find_next_siblings():\n",
        "            if sibling.name in ['h2', 'h3']:\n",
        "                break\n",
        "            section_content += sibling.get_text(separator=\" \", strip=True) + \" \"\n",
        "        if section_heading and section_content.strip():\n",
        "            sections.append((section_heading, section_content.strip()))\n",
        "    return sections\n",
        "\n",
        "def extract_tables(soup):\n",
        "    tables_data = []\n",
        "    tables = soup.find_all(\"table\")\n",
        "    for table in tables:\n",
        "        rows = table.find_all(\"tr\")\n",
        "        for row in rows:\n",
        "            cols = row.find_all([\"td\", \"th\"])\n",
        "            row_text = [col.get_text(strip=True) for col in cols]\n",
        "            if row_text:\n",
        "                tables_data.append(row_text)\n",
        "    return tables_data\n"
      ],
      "metadata": {
        "id": "yHjNfQqM1zMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save Data to DB**"
      ],
      "metadata": {
        "id": "S1L7Il1Q2BxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_page_data(conn, url, title, content):\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\"INSERT OR IGNORE INTO pages (url, title, content) VALUES (?, ?, ?)\", (url, title, content))\n",
        "    conn.commit()\n",
        "\n",
        "def save_section_data(conn, url, sections):\n",
        "    cursor = conn.cursor()\n",
        "    for heading, content in sections:\n",
        "        cursor.execute(\"INSERT INTO page_sections (url, section_heading, section_content) VALUES (?, ?, ?)\",\n",
        "                       (url, heading, content))\n",
        "    conn.commit()\n",
        "\n",
        "def save_table_data(conn, url, tables_data):\n",
        "    cursor = conn.cursor()\n",
        "    for row in tables_data:\n",
        "        cursor.execute(\"INSERT INTO page_tables (url, row_content) VALUES (?, ?)\", (url, json.dumps(row)))\n",
        "    conn.commit()"
      ],
      "metadata": {
        "id": "CM2Mf9H82A-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Code for Execution**"
      ],
      "metadata": {
        "id": "iiEO9RZo2Lc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    sitemap_url = \"https://www.mahindrauniversity.edu.in/sitemap_index.xml\"\n",
        "    db_path = \"mahindra_university_data.db\"\n",
        "    conn = setup_db(db_path)\n",
        "\n",
        "    print(\"Parsing sitemap...\")\n",
        "    all_urls = extract_all_urls(sitemap_url)\n",
        "    print(f\"Total URLs found: {len(all_urls)}\")\n",
        "\n",
        "    new_urls = [url for url in all_urls if not is_url_in_db(conn, url)]\n",
        "    print(f\"URLs to scrape: {len(new_urls)}\")\n",
        "\n",
        "    for url in tqdm(new_urls, desc=\"Scraping\"):\n",
        "        try:\n",
        "            res = requests.get(url, timeout=10)\n",
        "            if res.status_code == 200:\n",
        "                soup = BeautifulSoup(res.text, \"lxml\")\n",
        "                title = soup.title.string.strip() if soup.title else \"\"\n",
        "                full_content = extract_main_content(soup)\n",
        "\n",
        "                save_page_data(conn, url, title, full_content)\n",
        "\n",
        "                sections = extract_sections(soup)\n",
        "                if sections:\n",
        "                    save_section_data(conn, url, sections)\n",
        "\n",
        "                tables_data = extract_tables(soup)\n",
        "                if tables_data:\n",
        "                    save_table_data(conn, url, tables_data)\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Scraping {url}: {e}\")\n",
        "\n",
        "    print(\"Scraping complete. Page, section, and table content stored in SQLite.\")\n"
      ],
      "metadata": {
        "id": "QkSPKt0P2A3d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}